{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization for LLM\n",
        "1. Split text inot words and sub words (Token)\n",
        "2. Convert/Assign each token to token id\n",
        "\n",
        "* Book for data (The Verdict) - https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\n"
      ],
      "metadata": {
        "id": "H05XKUff7s2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Creating Tokens"
      ],
      "metadata": {
        "id": "RaSQI1B-aZFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import re"
      ],
      "metadata": {
        "id": "hMZPje8r9rqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFKYer-i68dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7ae9ff0-f975-400d-9208-133689089a28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 148208 characters\n",
            "First 100 characters of text: TITLE: Alice's Adventures in Wonderland\n",
            "AUTHOR: Lewis Carroll\n",
            "\n",
            "\n",
            "= CHAPTER I = \n",
            "=( Down the Rabbit-H\n"
          ]
        }
      ],
      "source": [
        "# Download the book\n",
        "# file_path = \"the-verdict.txt\"\n",
        "file_path = \"alice_in_wonderland.txt\"\n",
        "# url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
        "url = \"https://raw.githubusercontent.com/kuemit/txt_book/refs/heads/master/examples/alice_in_wonderland.txt\"\n",
        "if not os.path.exists(file_path):\n",
        "  response = requests.get(url, timeout=30)\n",
        "  response.raise_for_status()\n",
        "  text_data = response.text\n",
        "  with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(text_data)\n",
        "else:\n",
        "  with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    text_data = file.read()\n",
        "\n",
        "print(f\"Length of text: {len(text_data)} characters\")\n",
        "print(f\"First 100 characters of text: {text_data[:99]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spliting the word\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text_data)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68hsm3OjY4Ju",
        "outputId": "f21d5279-185e-4a4f-b734-09fe9a18e53d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['TITLE', ':', 'Alice', \"'\", 's', 'Adventures', 'in', 'Wonderland', 'AUTHOR', ':', 'Lewis', 'Carroll', '=', 'CHAPTER', 'I', '=', '=', '(', 'Down', 'the', 'Rabbit-Hole', ')', '=', 'Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2anS5BYvaMY9",
        "outputId": "700ede96-6ef2-46ca-b84f-3ff9462a1bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Creating Token IDs"
      ],
      "metadata": {
        "id": "2aBGhufL-Dmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(list(set(preprocessed)))\n",
        "vcab_size = len(all_words)\n",
        "print(f\"Vocab size: {vcab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOrLGF1rcUOH",
        "outputId": "1e8fbddd-cd89-41b8-d8c8-ed0de8b9f15d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 3189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vcab = {word: i for i, word in enumerate(all_words)}\n",
        "print(vcab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZXduysjc1HN",
        "outputId": "73e9781c-ad0b-43ce-f677-707b6d8f1260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'!': 0, '\"': 1, \"'\": 2, '(': 3, ')': 4, '*': 5, ',': 6, '--': 7, '.': 8, '0': 9, '00': 10, '10': 11, '124': 12, '1865-11-26': 13, '2': 14, '2021-03-08': 15, '2021-08-03': 16, '30': 17, '5': 18, '500': 19, '8': 20, '9': 21, ':': 22, ';': 23, '=': 24, '?': 25, '@': 26, 'A': 27, 'ALICE': 28, 'ALL': 29, 'AND': 30, 'ARE': 31, 'AT': 32, 'AUTHOR': 33, 'Ada': 34, 'Adventures': 35, 'Advice': 36, 'After': 37, 'Ah': 38, 'Alas': 39, 'Alice': 40, 'All': 41, 'Allow': 42, 'Always': 43, 'Ambition': 44, 'An': 45, 'And': 46, 'Ann': 47, 'Antipathies': 48, 'Arithmetic': 49, 'As': 50, 'At': 51, 'Atheling': 52, 'Australia': 53, 'BE': 54, 'BEE': 55, 'BEFORE': 56, 'BEG': 57, 'BEST': 58, 'BOOK': 59, 'BOOTS': 60, 'BUSY': 61, 'Be': 62, 'Beau': 63, 'Beautiful': 64, 'Because': 65, 'Before': 66, 'Besides': 67, 'Between': 68, 'Bill': 69, 'Birds': 70, 'Brandy': 71, 'But': 72, 'By': 73, 'C': 74, 'CAN': 75, 'CHAPTER': 76, 'CHORUS': 77, 'COULD': 78, 'COURT': 79, 'CURTSEYING': 80, 'Canary': 81, 'Canterbury': 82, 'Carroll': 83, 'Cat': 84, 'Caterpillar': 85, 'Caucus-Race': 86, 'Caucus-race': 87, 'Cheshire': 88, 'Christmas': 89, 'Classics': 90, 'Coils': 91, 'Come': 92, 'Coming': 93, 'Conqueror': 94, 'Crab': 95, 'Croquet-Ground': 96, 'D': 97, 'DOES': 98, 'DON': 99, 'DOTH': 100, 'Derision': 101, 'Digging': 102, 'Dinah': 103, 'Dinn': 104, 'Distraction': 105, 'Do': 106, 'Dodo': 107, 'Don': 108, 'Dormouse': 109, 'Down': 110, 'Drawling': 111, 'Drawling-master': 112, 'Duchess': 113, 'Duck': 114, 'END': 115, 'ESQ': 116, 'EVEN': 117, 'EVER': 118, 'Eaglet': 119, 'Edgar': 120, 'Edwin': 121, 'Either': 122, 'Elsie': 123, 'England': 124, 'English': 125, 'Even': 126, 'Everybody': 127, 'Everything': 128, 'Evidence': 129, 'Exactly': 130, 'FATHER': 131, 'FENDER': 132, 'FIT': 133, 'FOOT': 134, 'FROM': 135, 'FUL': 136, 'Fainting': 137, 'Father': 138, 'First': 139, 'Fish-Footman': 140, 'Five': 141, 'Footman': 142, 'For': 143, 'Forty-two': 144, 'France': 145, 'French': 146, 'Frog-Footman': 147, 'Fury': 148, 'GAVE': 149, 'Game': 150, 'Geography': 151, 'Go': 152, 'Good-bye': 153, 'Grammar': 154, 'Grief': 155, 'Gryphon': 156, 'HAD': 157, 'HATED': 158, 'HAVE': 159, 'HE': 160, 'HEARTHRUG': 161, 'HEARTS': 162, 'HER': 163, 'HERE': 164, 'HIGH': 165, 'HIM': 166, 'HIS': 167, 'HOW': 168, 'Half-past': 169, 'Hardly': 170, 'Hare': 171, 'Has': 172, 'Hatter': 173, 'He': 174, 'Heads': 175, 'Hearts': 176, 'Her': 177, 'Here': 178, 'Him': 179, 'His': 180, 'How': 181, 'However': 182, 'Hush': 183, 'I': 184, 'IF': 185, 'II': 186, 'III': 187, 'IN': 188, 'INSIDE': 189, 'IS': 190, 'IT': 191, 'ITS': 192, 'IV': 193, 'IX': 194, 'If': 195, 'Imagine': 196, 'Improve': 197, 'In': 198, 'Indeed': 199, 'Involved': 200, 'It': 201, 'Jack-in-the-box': 202, 'Just': 203, 'KING': 204, 'KNOW': 205, 'Keep': 206, 'King': 207, 'Kings': 208, 'Knave': 209, 'LEAVE': 210, 'LESS': 211, 'LITTLE': 212, 'LL': 213, 'LOVE': 214, 'Lacie': 215, 'Last': 216, 'Lastly': 217, 'Latin': 218, 'Latitude': 219, 'Laughing': 220, 'Let': 221, 'Lewis': 222, 'Like': 223, 'Little': 224, 'Lizard': 225, 'Lobster': 226, 'London': 227, 'Long': 228, 'Longitude': 229, 'Lory': 230, 'Luckily': 231, 'M': 232, 'MARMALADE': 233, 'ME': 234, 'MILE': 235, 'MINE': 236, 'MORE': 237, 'MUST': 238, 'MYSELF': 239, 'Ma': 240, 'Mabel': 241, 'Mad': 242, 'Magpie': 243, 'Majesty': 244, 'March': 245, 'Mary': 246, 'May': 247, 'Maybe': 248, 'Mercia': 249, 'Mind': 250, 'Miss': 251, 'Mock': 252, 'Morcar': 253, 'Mouse': 254, 'Multiplication': 255, 'My': 256, 'Mystery': 257, 'NEAR': 258, 'NEVER': 259, 'NO': 260, 'NOT': 261, 'Nay': 262, 'Never': 263, 'New': 264, 'Next': 265, 'Nile': 266, 'No': 267, 'Nobody': 268, 'Normans': 269, 'Northumbria': 270, 'Now': 271, 'Number': 272, 'O': 273, 'OF': 274, 'OLD': 275, 'ONE': 276, 'OURS': 277, 'OUT': 278, 'OUTSIDE': 279, 'Off': 280, 'Oh': 281, 'On': 282, 'Once': 283, 'One': 284, 'Only': 285, 'Our': 286, 'Owl': 287, 'PERSONS': 288, 'PLEASE': 289, 'PLENTY': 290, 'POCKET': 291, 'PRECIOUS': 292, 'PROVES': 293, 'PUB-DATE': 294, 'Panther': 295, 'Paris': 296, 'Pat': 297, 'Pennyworth': 298, 'Pepper': 299, 'Pig': 300, 'Pigeon': 301, 'Pinch': 302, 'Please': 303, 'Pool': 304, 'Poor': 305, 'Pray': 306, 'Presently': 307, 'Prizes': 308, 'Puss': 309, 'QUEEN': 310, 'QUITE': 311, 'Quadrille': 312, 'Queen': 313, 'Queens': 314, 'Quick': 315, 'RABBIT': 316, 'RED': 317, 'RETURNED': 318, 'RIGHT': 319, 'Rabbit': 320, 'Rabbit-Hole': 321, 'Rome': 322, 'Run': 323, 'S': 324, 'SAID': 325, 'SHE': 326, 'SHOES': 327, 'SIT': 328, 'SLUGGARD': 329, 'SOME': 330, 'SOMEBODY': 331, 'SOMETHING': 332, 'SOMEWHERE': 333, 'SOUP': 334, 'SWIM': 335, 'Said': 336, 'Seaography': 337, 'See': 338, 'Sends': 339, 'Serpent': 340, 'Seven': 341, 'Shakespeare': 342, 'Shall': 343, 'Shark': 344, 'She': 345, 'Silence': 346, 'Sing': 347, 'Sir': 348, 'So': 349, 'Some': 350, 'Soo': 351, 'Soon': 352, 'Sounds': 353, 'Soup': 354, 'Stigand': 355, 'Still': 356, 'Stole': 357, 'Stop': 358, 'Story': 359, 'Stretching': 360, 'Such': 361, 'Suddenly': 362, 'Suppress': 363, 'T': 364, 'THAN': 365, 'THAT': 366, 'THE': 367, 'THEIR': 368, 'THEN': 369, 'THERE': 370, 'THESE': 371, 'THEY': 372, 'THINK': 373, 'THIS': 374, 'THROUGH': 375, 'TIS': 376, 'TITLE': 377, 'TO': 378, 'TOOK': 379, 'TRUE': 380, 'TWO': 381, 'TXT': 382, 'Table': 383, 'Take': 384, 'Tale': 385, 'Tarts': 386, 'Tea-Party': 387, 'Tears': 388, 'Tell': 389, 'That': 390, 'The': 391, 'Then': 392, 'There': 393, 'Therefore': 394, 'These': 395, 'They': 396, 'This': 397, 'Those': 398, 'Though': 399, 'Tillie': 400, 'Time': 401, 'Tis': 402, 'Too': 403, 'Tortoise': 404, 'Trims': 405, 'Turn': 406, 'Turtle': 407, 'Twinkle': 408, 'Two': 409, 'Ugh': 410, 'Uglification': 411, 'Up': 412, 'V': 413, 'VE': 414, 'VERY': 415, 'VI': 416, 'VII': 417, 'VIII': 418, 'VOICE': 419, 'Very': 420, 'Visit': 421, 'WAISTCOAT-': 422, 'WAS': 423, 'WASHING': 424, 'WATCH': 425, 'WE': 426, 'WHAT': 427, 'WHATEVER': 428, 'WILL': 429, 'WILLIAM': 430, 'WITH': 431, 'WOULD': 432, 'Waiting': 433, 'Was': 434, 'We': 435, 'What': 436, 'When': 437, 'Where': 438, 'Which': 439, 'While': 440, 'White': 441, 'Who': 442, 'Why': 443, 'Will': 444, 'William': 445, 'With': 446, 'Wonderland': 447, 'Would': 448, 'Writhing': 449, 'X': 450, 'XI': 451, 'XII': 452, 'YET': 453, 'YOU': 454, 'YOUR': 455, 'YOURS': 456, 'Yet': 457, 'You': 458, 'Zealand': 459, '[later': 460, ']': 461, '_': 462, '`': 463, '`A': 464, '`ARE': 465, '`After': 466, '`Ah': 467, '`Ahem': 468, '`Alice': 469, '`All': 470, '`An': 471, '`And': 472, '`Anything': 473, '`Are': 474, '`As': 475, '`At': 476, '`Back': 477, '`Beautiful': 478, '`Begin': 479, '`Behead': 480, '`Boots': 481, '`Bring': 482, '`But': 483, '`By-the-bye': 484, '`Call': 485, '`Can': 486, '`Catch': 487, '`Certainly': 488, '`Change': 489, '`Cheshire': 490, '`Chorus': 491, '`Collar': 492, '`Come': 493, '`Consider': 494, '`Curiouser': 495, '`DRINK': 496, '`Dear': 497, '`Did': 498, '`Digging': 499, '`Dinah': 500, '`Do': 501, '`Does': 502, '`Don': 503, '`Drink': 504, '`Drive': 505, '`EAT': 506, '`EVERYBODY': 507, '`Each': 508, '`Everybody': 509, '`Everything': 510, '`Exactly': 511, '`Explain': 512, '`Fetch': 513, '`Fifteenth': 514, '`First': 515, '`For': 516, '`Found': 517, '`Fourteenth': 518, '`From': 519, '`Fury': 520, '`Get': 521, '`Give': 522, '`Go': 523, '`HE': 524, '`Hadn': 525, '`Hand': 526, '`Have': 527, '`He': 528, '`Herald': 529, '`Here': 530, '`Hjckrrh': 531, '`Hm': 532, '`Hold': 533, '`How': 534, '`Hush': 535, '`I': 536, '`IF': 537, '`IT': 538, '`Idiot': 539, '`If': 540, '`In': 541, '`Is': 542, '`It': 543, '`Just': 544, '`Keep': 545, '`Leave': 546, '`Let': 547, '`Look': 548, '`Mary': 549, '`May': 550, '`Mine': 551, '`Mouse': 552, '`My': 553, '`Nearly': 554, '`Never': 555, '`No': 556, '`Nobody': 557, '`Nonsense': 558, '`Nor': 559, '`Not': 560, '`Nothing': 561, '`Now': 562, '`O': 563, '`ORANGE': 564, '`Of': 565, '`Off': 566, '`Oh': 567, '`Once': 568, '`One': 569, '`Only': 570, '`Or': 571, '`Ou': 572, '`Pat': 573, '`Pepper': 574, '`Perhaps': 575, '`Please': 576, '`Poor': 577, '`Pray': 578, '`Prizes': 579, '`Read': 580, '`Really': 581, '`Reeling': 582, '`Repeat': 583, '`Right': 584, '`Rule': 585, '`SHE': 586, '`Same': 587, '`Seals': 588, '`Sentence': 589, '`Serpent': 590, '`Seven': 591, '`Sh': 592, '`Shall': 593, '`Shan': 594, '`She': 595, '`Shy': 596, '`Silence': 597, '`Sit': 598, '`Sixteenth': 599, '`So': 600, '`Soles': 601, '`Somebody': 602, '`Soo': 603, '`Speak': 604, '`Stand': 605, '`Stolen': 606, '`Stuff': 607, '`Stupid': 608, '`Suppose': 609, '`Sure': 610, '`Swim': 611, '`THAT': 612, '`Take': 613, '`Talking': 614, '`Tell': 615, '`Ten': 616, '`Thank': 617, '`That': 618, '`The': 619, '`Their': 620, '`Then': 621, '`There': 622, '`They': 623, '`Thinking': 624, '`This': 625, '`To': 626, '`Treacle': 627, '`Turn': 628, '`Tut': 629, '`Twenty-four': 630, '`Twinkle': 631, '`Two': 632, '`UNimportant': 633, '`Ugh': 634, '`Up': 635, '`VERY': 636, '`Very': 637, '`W': 638, '`Wake': 639, '`We': 640, '`Well': 641, '`What': 642, '`When': 643, '`Where': 644, '`Which': 645, '`Who': 646, '`Whoever': 647, '`Why': 648, '`With': 649, '`Would': 650, '`Wouldn': 651, '`Wow': 652, '`Write': 653, '`YOU': 654, '`Yes': 655, '`You': 656, '`Your': 657, '`a': 658, '`advance': 659, '`after': 660, '`all': 661, '`allow': 662, '`and': 663, '`any': 664, '`are': 665, '`arrum': 666, '`as': 667, '`at': 668, '`because': 669, '`besides': 670, '`but': 671, '`chop': 672, '`creatures': 673, '`crumbs': 674, '`don': 675, '`either': 676, '`explanations': 677, '`fetch': 678, '`flamingoes': 679, '`for': 680, '`he': 681, '`his': 682, '`how': 683, '`however': 684, '`if': 685, '`important': 686, '`in': 687, '`is': 688, '`it': 689, '`jury-men': 690, '`just': 691, '`leave': 692, '`let': 693, '`lives': 694, '`living': 695, '`moral': 696, '`nine': 697, '`no': 698, '`not': 699, '`now': 700, '`of': 701, '`on': 702, '`one': 703, '`only': 704, '`or': 705, '`poison': 706, '`really': 707, '`shall': 708, '`she': 709, '`sit': 710, '`so': 711, '`some': 712, '`stupid': 713, '`tell': 714, '`than': 715, '`that': 716, '`the': 717, '`there': 718, '`they': 719, '`this': 720, '`three': 721, '`till': 722, '`to': 723, '`unimportant': 724, '`unless': 725, '`until': 726, '`was': 727, '`we': 728, '`what': 729, '`when': 730, '`whenever': 731, '`which': 732, '`why': 733, '`without': 734, '`you': 735, 'a': 736, 'a-piece': 737, 'abide': 738, 'able': 739, 'about': 740, 'above': 741, 'absence': 742, 'absurd': 743, 'acceptance': 744, 'accident': 745, 'accidentally': 746, 'account': 747, 'accounting': 748, 'accounts': 749, 'accusation': 750, 'accustomed': 751, 'ache': 752, 'across': 753, 'act': 754, 'actually': 755, 'added': 756, 'adding': 757, 'addressed': 758, 'addressing': 759, 'adjourn': 760, 'adoption': 761, 'advance': 762, 'advantage': 763, 'adventures': 764, 'advice': 765, 'advisable': 766, 'advise': 767, 'affair': 768, 'affectionately': 769, 'afford': 770, 'afore': 771, 'afraid': 772, 'after': 773, 'after-time': 774, 'afterwards': 775, 'again': 776, 'against': 777, 'age': 778, 'ago': 779, 'agony': 780, 'agree': 781, 'air': 782, 'airs': 783, 'aka': 784, 'alarm': 785, 'alarmed': 786, 'alas': 787, 'alive': 788, 'all': 789, 'allow': 790, 'almost': 791, 'alone': 792, 'along': 793, 'aloud': 794, 'already': 795, 'also': 796, 'altered': 797, 'alternately': 798, 'altogether': 799, 'always': 800, 'am': 801, 'among': 802, 'an': 803, 'ancient': 804, 'and': 805, 'and-butter': 806, 'anger': 807, 'angrily': 808, 'angry': 809, 'animal': 810, 'animals': 811, 'annoy': 812, 'annoyed': 813, 'another': 814, 'answer': 815, 'answered': 816, 'answers': 817, 'anxious': 818, 'anxiously': 819, 'any': 820, 'anything': 821, 'anywhere': 822, 'appealed': 823, 'appear': 824, 'appearance': 825, 'appeared': 826, 'appearing': 827, 'applause': 828, 'apples': 829, 'arch': 830, 'archbishop': 831, 'arches': 832, 'are': 833, 'argue': 834, 'argued': 835, 'argument': 836, 'arguments': 837, 'arm': 838, 'arm-chair': 839, 'arm-in-arm': 840, 'arms': 841, 'around': 842, 'arranged': 843, 'arrived': 844, 'arrow': 845, 'as': 846, 'ashamed': 847, 'ask': 848, 'askance': 849, 'asked': 850, 'asking': 851, 'asleep': 852, 'assembled': 853, 'at': 854, 'ate': 855, 'atom': 856, 'attempt': 857, 'attempted': 858, 'attempts': 859, 'attended': 860, 'attending': 861, 'attends': 862, 'audibly': 863, 'authority': 864, 'avoid': 865, 'away': 866, 'awfully': 867, 'axes': 868, 'axis': 869, 'baby': 870, 'back': 871, 'back-somersault': 872, 'backs': 873, 'bad': 874, 'bag': 875, 'baked': 876, 'balanced': 877, 'balls': 878, 'bank': 879, 'banks': 880, 'banquet': 881, 'bark': 882, 'barking': 883, 'barley-sugar': 884, 'barrowful': 885, 'bat': 886, 'bathing': 887, 'bats': 888, 'bawled': 889, 'be': 890, 'beak': 891, 'bear': 892, 'beast': 893, 'beasts': 894, 'beat': 895, 'beating': 896, 'beauti': 897, 'beautiful': 898, 'beautifully': 899, 'beautify': 900, 'became': 901, 'because': 902, 'become': 903, 'becoming': 904, 'bed': 905, 'beds': 906, 'been': 907, 'before': 908, 'beg': 909, 'began': 910, 'begged': 911, 'begin': 912, 'beginning': 913, 'begins': 914, 'begun': 915, 'beheaded': 916, 'beheading': 917, 'behind': 918, 'being': 919, 'believe': 920, 'believed': 921, 'bells': 922, 'belong': 923, 'belongs': 924, 'beloved': 925, 'below': 926, 'belt': 927, 'bend': 928, 'bent': 929, 'besides': 930, 'best': 931, 'better': 932, 'between': 933, 'bill': 934, 'bird': 935, 'birds': 936, 'birthday': 937, 'bit': 938, 'bite': 939, 'bitter': 940, 'blacking': 941, 'blades': 942, 'blame': 943, 'blasts': 944, 'bleeds': 945, 'blew': 946, 'blow': 947, 'blown': 948, 'blows': 949, 'body': 950, 'boldly': 951, 'bone': 952, 'bones': 953, 'book': 954, 'book-shelves': 955, 'bookmark': 956, 'books': 957, 'boon': 958, 'boots': 959, 'bore': 960, 'both': 961, 'bother': 962, 'bottle': 963, 'bottom': 964, 'bough': 965, 'bound': 966, 'bowed': 967, 'bowing': 968, 'box': 969, 'boxed': 970, 'boy': 971, 'brain': 972, 'branch': 973, 'branches': 974, 'brass': 975, 'brave': 976, 'bread-': 977, 'bread-and-butter': 978, 'bread-knife': 979, 'break': 980, 'breath': 981, 'breathe': 982, 'breeze': 983, 'bright': 984, 'bright-eyed': 985, 'brightened': 986, 'bring': 987, 'bringing': 988, 'bristling': 989, 'broke': 990, 'broken': 991, 'brother': 992, 'brought': 993, 'brown': 994, 'brush': 995, 'brushing': 996, 'burn': 997, 'burning': 998, 'burnt': 999, 'burst': 1000, 'bursting': 1001, 'busily': 1002, 'business': 1003, 'busy': 1004, 'but': 1005, 'butter': 1006, 'buttercup': 1007, 'buttered': 1008, 'butterfly': 1009, 'buttons': 1010, 'by': 1011, 'cackled': 1012, 'cake': 1013, 'cakes': 1014, 'call': 1015, 'called': 1016, 'calling': 1017, 'calmly': 1018, 'came': 1019, 'camomile': 1020, 'can': 1021, 'candle': 1022, 'cannot': 1023, 'canvas': 1024, 'capering': 1025, 'capital': 1026, 'cardboard': 1027, 'cards': 1028, 'care': 1029, 'carefully': 1030, 'cares': 1031, 'carried': 1032, 'carrier': 1033, 'carry': 1034, 'carrying': 1035, 'cart-horse': 1036, 'cartwheels': 1037, 'case': 1038, 'cat': 1039, 'catch': 1040, 'catching': 1041, 'caterpillar': 1042, 'cats': 1043, 'cattle': 1044, 'caught': 1045, 'cauldron': 1046, 'cause': 1047, 'caused': 1048, 'cautiously': 1049, 'ceiling': 1050, 'centre': 1051, 'certain': 1052, 'certainly': 1053, 'chains': 1054, 'chance': 1055, 'chanced': 1056, 'change': 1057, 'changed': 1058, 'changes': 1059, 'changing': 1060, 'character': 1061, 'charges': 1062, 'chatte': 1063, 'cheap': 1064, 'cheated': 1065, 'checked': 1066, 'cheeks': 1067, 'cheered': 1068, 'cheerfully': 1069, 'cherry-tart': 1070, 'chief': 1071, 'child': 1072, 'child-life': 1073, 'childhood': 1074, 'children': 1075, 'chimney': 1076, 'chimneys': 1077, 'chin': 1078, 'choice': 1079, 'choke': 1080, 'choked': 1081, 'choking': 1082, 'choosing': 1083, 'chorus': 1084, 'chose': 1085, 'chrysalis': 1086, 'chuckled': 1087, 'circle': 1088, 'circumstances': 1089, 'civil': 1090, 'clamour': 1091, 'clapping': 1092, 'clasped': 1093, 'claws': 1094, 'clean': 1095, 'clear': 1096, 'cleared': 1097, 'clearer': 1098, 'clearly': 1099, 'clever': 1100, 'climb': 1101, 'clinging': 1102, 'clock': 1103, 'close': 1104, 'closed': 1105, 'closely': 1106, 'closer': 1107, 'clubs': 1108, 'coast': 1109, 'coaxing': 1110, 'cold': 1111, 'collected': 1112, 'come': 1113, 'comes': 1114, 'comfits': 1115, 'comfort': 1116, 'comfortable': 1117, 'comfortably': 1118, 'coming': 1119, 'common': 1120, 'commotion': 1121, 'company': 1122, 'complained': 1123, 'complaining': 1124, 'completely': 1125, 'concert': 1126, 'concluded': 1127, 'conclusion': 1128, 'condemn': 1129, 'conduct': 1130, 'confused': 1131, 'confusing': 1132, 'confusion': 1133, 'conger-eel': 1134, 'conquest': 1135, 'consented': 1136, 'consider': 1137, 'considered': 1138, 'considering': 1139, 'constant': 1140, 'consultation': 1141, 'contempt': 1142, 'contemptuous': 1143, 'contemptuously': 1144, 'content': 1145, 'continued': 1146, 'contradicted': 1147, 'conversation': 1148, 'conversations': 1149, 'cook': 1150, 'cool': 1151, 'corner': 1152, 'corners': 1153, 'cost': 1154, 'could': 1155, 'couldn': 1156, 'counting': 1157, 'country': 1158, 'couple': 1159, 'couples': 1160, 'courage': 1161, 'course': 1162, 'court': 1163, 'courtiers': 1164, 'coward': 1165, 'crab': 1166, 'crash': 1167, 'crashed': 1168, 'crawled': 1169, 'crawling': 1170, 'crazy': 1171, 'creature': 1172, 'creatures': 1173, 'creep': 1174, 'crept': 1175, 'cried': 1176, 'cries': 1177, 'crimson': 1178, 'crocodile': 1179, 'croquet': 1180, 'croquet-ground': 1181, 'croqueted': 1182, 'croqueting': 1183, 'cross': 1184, 'cross-examine': 1185, 'crossed': 1186, 'crossly': 1187, 'crouched': 1188, 'crowd': 1189, 'crowded': 1190, 'crown': 1191, 'crumbs': 1192, 'cry': 1193, 'crying': 1194, 'cucumber-frame': 1195, 'cucumber-frames': 1196, 'cunning': 1197, 'cup': 1198, 'cupboards': 1199, 'cur': 1200, 'curiosity': 1201, 'curious': 1202, 'curiouser': 1203, 'curled': 1204, 'curls': 1205, 'curly': 1206, 'currants': 1207, 'curtain': 1208, 'curtsey': 1209, 'curving': 1210, 'cushion': 1211, 'custard': 1212, 'custody': 1213, 'cut': 1214, 'cutting': 1215, 'd': 1216, 'dainties': 1217, 'daisies': 1218, 'daisy-chain': 1219, 'dance': 1220, 'dancing': 1221, 'dare': 1222, 'daresay': 1223, 'dark': 1224, 'darkness': 1225, 'dates': 1226, 'daughter': 1227, 'day': 1228, 'day-school': 1229, 'days': 1230, 'dead': 1231, 'deal': 1232, 'dear': 1233, 'dears': 1234, 'death': 1235, 'decided': 1236, 'decidedly': 1237, 'declare': 1238, 'declared': 1239, 'deep': 1240, 'deepest': 1241, 'deeply': 1242, 'default': 1243, 'delay': 1244, 'delight': 1245, 'delighted': 1246, 'delightful': 1247, 'denial': 1248, 'denied': 1249, 'denies': 1250, 'deny': 1251, 'denying': 1252, 'depends': 1253, 'deserved': 1254, 'despair': 1255, 'desperate': 1256, 'desperately': 1257, 'diamonds': 1258, 'did': 1259, 'didn': 1260, 'die': 1261, 'died': 1262, 'different': 1263, 'difficult': 1264, 'difficulties': 1265, 'difficulty': 1266, 'dig': 1267, 'digging': 1268, 'diligently': 1269, 'dinn': 1270, 'dinner': 1271, 'dipped': 1272, 'directed': 1273, 'direction': 1274, 'directions': 1275, 'directly': 1276, 'disagree': 1277, 'disappeared': 1278, 'disappointment': 1279, 'disgust': 1280, 'dish': 1281, 'dishes': 1282, 'dismay': 1283, 'disobey': 1284, 'dispute': 1285, 'distance': 1286, 'distant': 1287, 'dive': 1288, 'do': 1289, 'dodged': 1290, 'does': 1291, 'doesn': 1292, 'dog': 1293, 'dogs': 1294, 'doing': 1295, 'don': 1296, 'done': 1297, 'door': 1298, 'doors': 1299, 'doorway': 1300, 'doth': 1301, 'double': 1302, 'doubled-up': 1303, 'doubling': 1304, 'doubt': 1305, 'doubtful': 1306, 'doubtfully': 1307, 'down': 1308, 'downward': 1309, 'downwards': 1310, 'doze': 1311, 'dozing': 1312, 'draggled': 1313, 'draw': 1314, 'drawing': 1315, 'dreadful': 1316, 'dreadfully': 1317, 'dream': 1318, 'dreamed': 1319, 'dreaming': 1320, 'dreamy': 1321, 'dressed': 1322, 'drew': 1323, 'dried': 1324, 'driest': 1325, 'drink': 1326, 'drinking': 1327, 'dripping': 1328, 'drive': 1329, 'drop': 1330, 'dropped': 1331, 'dropping': 1332, 'drowned': 1333, 'drunk': 1334, 'dry': 1335, 'duck': 1336, 'dull': 1337, 'dunce': 1338, 'e': 1339, 'each': 1340, 'eager': 1341, 'eagerly': 1342, 'ear': 1343, 'earls': 1344, 'earnestly': 1345, 'ears': 1346, 'earth': 1347, 'easily': 1348, 'easy': 1349, 'eat': 1350, 'eaten': 1351, 'eating': 1352, 'eats': 1353, 'edge': 1354, 'editions': 1355, 'educations': 1356, 'eel': 1357, 'eels': 1358, 'effect': 1359, 'egg': 1360, 'eggs': 1361, 'eh': 1362, 'either': 1363, 'elbow': 1364, 'elbows': 1365, 'elegant': 1366, 'eleventh': 1367, 'else': 1368, 'em': 1369, 'emphasis': 1370, 'empty': 1371, 'encourage': 1372, 'encouraged': 1373, 'encouraging': 1374, 'end': 1375, 'ending': 1376, 'energetic': 1377, 'engaged': 1378, 'engraved': 1379, 'enjoy': 1380, 'enormous': 1381, 'enough': 1382, 'entangled': 1383, 'entirely': 1384, 'entrance': 1385, 'escape': 1386, 'est': 1387, 'even': 1388, 'evening': 1389, 'ever': 1390, 'every': 1391, 'everybody': 1392, 'everything': 1393, 'evidence': 1394, 'evidently': 1395, 'exact': 1396, 'exactly': 1397, 'examining': 1398, 'excellent': 1399, 'except': 1400, 'exclaimed': 1401, 'exclamation': 1402, 'execute': 1403, 'executed': 1404, 'executes': 1405, 'execution': 1406, 'executioner': 1407, 'executions': 1408, 'existence': 1409, 'expected': 1410, 'expecting': 1411, 'experiment': 1412, 'explain': 1413, 'explained': 1414, 'explanation': 1415, 'expressing': 1416, 'expression': 1417, 'extra': 1418, 'extraordinary': 1419, 'extras': 1420, 'extremely': 1421, 'eye': 1422, 'eyelids': 1423, 'eyes': 1424, 'face': 1425, 'faces': 1426, 'fact': 1427, 'fading': 1428, 'failure': 1429, 'faint': 1430, 'faintly': 1431, 'fair': 1432, 'fairly': 1433, 'fairy-tales': 1434, 'fall': 1435, 'fallen': 1436, 'falling': 1437, 'familiarly': 1438, 'family': 1439, 'fan': 1440, 'fancied': 1441, 'fancy': 1442, 'fancying': 1443, 'fanned': 1444, 'fanning': 1445, 'far': 1446, 'farm-yard': 1447, 'farmer': 1448, 'farther': 1449, 'fashion': 1450, 'fast': 1451, 'faster': 1452, 'fat': 1453, 'father': 1454, 'favoured': 1455, 'favourite': 1456, 'fear': 1457, 'feared': 1458, 'feather': 1459, 'feathers': 1460, 'feeble': 1461, 'feebly': 1462, 'feel': 1463, 'feeling': 1464, 'feelings': 1465, 'feet': 1466, 'fell': 1467, 'fellow': 1468, 'fellows': 1469, 'felt': 1470, 'ferrets': 1471, 'fetch': 1472, 'few': 1473, 'fidgeted': 1474, 'field': 1475, 'fifteen': 1476, 'fifth': 1477, 'fig': 1478, 'fight': 1479, 'fighting': 1480, 'figure': 1481, 'figures': 1482, 'filled': 1483, 'fills': 1484, 'find': 1485, 'finding': 1486, 'finds': 1487, 'fine': 1488, 'finger': 1489, 'finish': 1490, 'finished': 1491, 'finishing': 1492, 'fire': 1493, 'fire-irons': 1494, 'fireplace': 1495, 'first': 1496, 'fish': 1497, 'fishes': 1498, 'fit': 1499, 'fits': 1500, 'fitted': 1501, 'five': 1502, 'fix': 1503, 'fixed': 1504, 'flame': 1505, 'flamingo': 1506, 'flamingoes': 1507, 'flapper': 1508, 'flappers': 1509, 'flashed': 1510, 'flat': 1511, 'flavour': 1512, 'flew': 1513, 'flinging': 1514, 'flock': 1515, 'floor': 1516, 'flower-beds': 1517, 'flower-pot': 1518, 'flowers': 1519, 'flown': 1520, 'flung': 1521, 'flurry': 1522, 'flustered': 1523, 'fluttered': 1524, 'fly': 1525, 'flying': 1526, 'folded': 1527, 'folding': 1528, 'follow': 1529, 'followed': 1530, 'follows': 1531, 'fond': 1532, 'foolish': 1533, 'foot': 1534, 'footman': 1535, 'footmen': 1536, 'footsteps': 1537, 'for': 1538, 'forehead': 1539, 'forepaws': 1540, 'forget': 1541, 'forgetting': 1542, 'forgot': 1543, 'forgotten': 1544, 'fork': 1545, 'form': 1546, 'fortunately': 1547, 'forwards': 1548, 'found': 1549, 'fountains': 1550, 'four': 1551, 'fourth': 1552, 'free': 1553, 'friend': 1554, 'friends': 1555, 'fright': 1556, 'frighten': 1557, 'frightened': 1558, 'frog': 1559, 'from': 1560, 'front': 1561, 'frontispiece': 1562, 'frowning': 1563, 'frying-pan': 1564, 'full': 1565, 'fumbled': 1566, 'fun': 1567, 'funny': 1568, 'fur': 1569, 'furious': 1570, 'furiously': 1571, 'furrow': 1572, 'furrows': 1573, 'further': 1574, 'fury': 1575, 'gained': 1576, 'gallons': 1577, 'game': 1578, 'games': 1579, 'garden': 1580, 'gardeners': 1581, 'gather': 1582, 'gave': 1583, 'gay': 1584, 'gazing': 1585, 'general': 1586, 'generally': 1587, 'gently': 1588, 'get': 1589, 'getting': 1590, 'giddy': 1591, 'girl': 1592, 'girls': 1593, 'give': 1594, 'given': 1595, 'giving': 1596, 'glad': 1597, 'glanced': 1598, 'glaring': 1599, 'glass': 1600, 'globe': 1601, 'gloomily': 1602, 'gloves': 1603, 'go': 1604, 'goes': 1605, 'going': 1606, 'golden': 1607, 'goldfish': 1608, 'gone': 1609, 'good': 1610, 'good-': 1611, 'good-naturedly': 1612, 'goose': 1613, 'got': 1614, 'graceful': 1615, 'grand': 1616, 'grant': 1617, 'grass': 1618, 'grave': 1619, 'gravely': 1620, 'gravy': 1621, 'grazed': 1622, 'great': 1623, 'green': 1624, 'grew': 1625, 'grey': 1626, 'grin': 1627, 'grinned': 1628, 'grinning': 1629, 'grins': 1630, 'ground': 1631, 'grow': 1632, 'growing': 1633, 'growl': 1634, 'growled': 1635, 'growling': 1636, 'growls': 1637, 'grown': 1638, 'grumbled': 1639, 'grunt': 1640, 'grunted': 1641, 'guard': 1642, 'guess': 1643, 'guessed': 1644, 'guests': 1645, 'guilt': 1646, 'guinea-pig': 1647, 'guinea-pigs': 1648, 'had': 1649, 'hadn': 1650, 'hair': 1651, 'half': 1652, 'half-past': 1653, 'hall': 1654, 'hand': 1655, 'handed': 1656, 'hands': 1657, 'handsome': 1658, 'handwriting': 1659, 'hanging': 1660, 'happen': 1661, 'happened': 1662, 'happening': 1663, 'happens': 1664, 'happy': 1665, 'hard': 1666, 'hardly': 1667, 'harm': 1668, 'has': 1669, 'hasn': 1670, 'haste': 1671, 'hastily': 1672, 'hat': 1673, 'hatching': 1674, 'hate': 1675, 'hatter': 1676, 'hatters': 1677, 'have': 1678, 'haven': 1679, 'having': 1680, 'he': 1681, 'head': 1682, 'heads': 1683, 'heap': 1684, 'hear': 1685, 'heard': 1686, 'hearing': 1687, 'heart': 1688, 'hearth': 1689, 'hearts': 1690, 'heavy': 1691, 'hedge': 1692, 'hedgehog': 1693, 'hedgehogs': 1694, 'hedges': 1695, 'heels': 1696, 'height': 1697, 'held': 1698, 'help': 1699, 'helped': 1700, 'helpless': 1701, 'her': 1702, 'here': 1703, 'hers': 1704, 'herself': 1705, 'hid': 1706, 'hide': 1707, 'high': 1708, 'highest': 1709, 'him': 1710, 'himself': 1711, 'hint': 1712, 'hippopotamus': 1713, 'his': 1714, 'hiss': 1715, 'histories': 1716, 'history': 1717, 'hit': 1718, 'hoarse': 1719, 'hoarsely': 1720, 'hold': 1721, 'holding': 1722, 'holiday': 1723, 'hollow': 1724, 'home': 1725, 'honest': 1726, 'honour': 1727, 'hookah': 1728, 'hope': 1729, 'hoped': 1730, 'hopeful': 1731, 'hopeless': 1732, 'hoping': 1733, 'hot': 1734, 'hot-tempered': 1735, 'hour': 1736, 'hours': 1737, 'house': 1738, 'housemaid': 1739, 'houses': 1740, 'how': 1741, 'however': 1742, 'howled': 1743, 'howling': 1744, 'humble': 1745, 'humbly': 1746, 'hundred': 1747, 'hung': 1748, 'hungry': 1749, 'hunting': 1750, 'hurried': 1751, 'hurriedly': 1752, 'hurry': 1753, 'hurrying': 1754, 'hurt': 1755, 'hush': 1756, 'idea': 1757, 'idiotic': 1758, 'if': 1759, 'ignorant': 1760, 'ill': 1761, 'imagine': 1762, 'imitated': 1763, 'immediate': 1764, 'immediately': 1765, 'immense': 1766, 'impatient': 1767, 'impatiently': 1768, 'impertinent': 1769, 'important': 1770, 'impossible': 1771, 'in': 1772, 'incessantly': 1773, 'inches': 1774, 'inclined': 1775, 'indeed': 1776, 'indignant': 1777, 'indignantly': 1778, 'injure': 1779, 'ink': 1780, 'inkstand': 1781, 'inquired': 1782, 'inquisitively': 1783, 'inside': 1784, 'insolence': 1785, 'instance': 1786, 'instantly': 1787, 'instead': 1788, 'insult': 1789, 'interest': 1790, 'interesting': 1791, 'interrupt': 1792, 'interrupted': 1793, 'interrupting': 1794, 'into': 1795, 'introduce': 1796, 'introduced': 1797, 'invent': 1798, 'invented': 1799, 'invitation': 1800, 'invited': 1801, 'inwards': 1802, 'irritated': 1803, 'is': 1804, 'isn': 1805, 'it': 1806, 'its': 1807, 'itself': 1808, 'jar': 1809, 'jaw': 1810, 'jaws': 1811, 'jelly-fish': 1812, 'jogged': 1813, 'join': 1814, 'joined': 1815, 'journey': 1816, 'joys': 1817, 'judge': 1818, 'judging': 1819, 'jumped': 1820, 'jumping': 1821, 'juror': 1822, 'jurors': 1823, 'jury': 1824, 'jury-box': 1825, 'jurymen': 1826, 'just': 1827, 'justice': 1828, 'keep': 1829, 'keeping': 1830, 'kept': 1831, 'kettle': 1832, 'key': 1833, 'kick': 1834, 'kid': 1835, 'kill': 1836, 'killing': 1837, 'kills': 1838, 'kind': 1839, 'kindly': 1840, 'king': 1841, 'kiss': 1842, 'kissed': 1843, 'kitchen': 1844, 'knee': 1845, 'kneel': 1846, 'knelt': 1847, 'knew': 1848, 'knife': 1849, 'knock': 1850, 'knocked': 1851, 'knocking': 1852, 'knot': 1853, 'know': 1854, 'knowing': 1855, 'knowledge': 1856, 'known': 1857, 'knows': 1858, 'knuckles': 1859, 'label': 1860, 'labelled': 1861, 'lad': 1862, 'ladder': 1863, 'lady': 1864, 'laid': 1865, 'lamps': 1866, 'land': 1867, 'languid': 1868, 'lap': 1869, 'large': 1870, 'larger': 1871, 'largest': 1872, 'lark': 1873, 'last': 1874, 'lasted': 1875, 'late': 1876, 'lately': 1877, 'later': 1878, 'laugh': 1879, 'laughed': 1880, 'laughing': 1881, 'laughter': 1882, 'law': 1883, 'lay': 1884, 'lazily': 1885, 'lazy': 1886, 'leaders': 1887, 'leading': 1888, 'leaning': 1889, 'leant': 1890, 'leap': 1891, 'learn': 1892, 'learned': 1893, 'learning': 1894, 'learnt': 1895, 'least': 1896, 'leave': 1897, 'leaves': 1898, 'leaving': 1899, 'led': 1900, 'ledge': 1901, 'left': 1902, 'lefthand': 1903, 'legs': 1904, 'length': 1905, 'less': 1906, 'lessen': 1907, 'lesson': 1908, 'lesson-book': 1909, 'lesson-books': 1910, 'lessons': 1911, 'lest': 1912, 'let': 1913, 'letter': 1914, 'letters': 1915, 'licking': 1916, 'lie': 1917, 'life': 1918, 'lifted': 1919, 'like': 1920, 'liked': 1921, 'likely': 1922, 'likes': 1923, 'limbs': 1924, 'line': 1925, 'lines': 1926, 'linked': 1927, 'lips': 1928, 'list': 1929, 'listen': 1930, 'listened': 1931, 'listeners': 1932, 'listening': 1933, 'lit': 1934, 'little': 1935, 'live': 1936, 'lived': 1937, 'livery': 1938, 'lives': 1939, 'living': 1940, 'll': 1941, 'lobster': 1942, 'lobsters': 1943, 'lock': 1944, 'locked': 1945, 'locks': 1946, 'lodging': 1947, 'lonely': 1948, 'long': 1949, 'longed': 1950, 'longer': 1951, 'look': 1952, 'look-out': 1953, 'looked': 1954, 'looking': 1955, 'looking-': 1956, 'loose': 1957, 'lose': 1958, 'losing': 1959, 'lost': 1960, 'loud': 1961, 'louder': 1962, 'loudly': 1963, 'love': 1964, 'loveliest': 1965, 'lovely': 1966, 'loving': 1967, 'low': 1968, 'low-spirited': 1969, 'lower': 1970, 'lowing': 1971, 'luckily': 1972, 'lullaby': 1973, 'lying': 1974, 'm': 1975, 'ma': 1976, 'machines': 1977, 'mad': 1978, 'made': 1979, 'magic': 1980, 'make': 1981, 'makes': 1982, 'making': 1983, 'mallets': 1984, 'man': 1985, 'manage': 1986, 'managed': 1987, 'managing': 1988, 'manner': 1989, 'manners': 1990, 'many': 1991, 'maps': 1992, 'marched': 1993, 'mark': 1994, 'marked': 1995, 'master': 1996, 'matter': 1997, 'matters': 1998, 'may': 1999, 'maybe': 2000, 'mayn': 2001, 'me': 2002, 'meal': 2003, 'mean': 2004, 'meaning': 2005, 'means': 2006, 'meant': 2007, 'meanwhile': 2008, 'measure': 2009, 'meat': 2010, 'meekly': 2011, 'meet': 2012, 'meeting': 2013, 'melancholy': 2014, 'memorandum': 2015, 'memory': 2016, 'mentioned': 2017, 'merely': 2018, 'merrily': 2019, 'messages': 2020, 'met': 2021, 'mice': 2022, 'middle': 2023, 'might': 2024, 'mile': 2025, 'miles': 2026, 'milk': 2027, 'milk-jug': 2028, 'mind': 2029, 'minded': 2030, 'minding': 2031, 'mine': 2032, 'mineral': 2033, 'minute': 2034, 'minutes': 2035, 'mischief': 2036, 'miserable': 2037, 'miss': 2038, 'missed': 2039, 'mistake': 2040, 'mixed': 2041, 'moderate': 2042, 'modern': 2043, 'moment': 2044, 'month': 2045, 'moon': 2046, 'moral': 2047, 'morals': 2048, 'more': 2049, 'morning': 2050, 'morsel': 2051, 'most': 2052, 'mostly': 2053, 'mournful': 2054, 'mournfully': 2055, 'mouse': 2056, 'mouse-traps': 2057, 'mouth': 2058, 'mouths': 2059, 'move': 2060, 'moved': 2061, 'moving': 2062, 'much': 2063, 'muchness': 2064, 'muddle': 2065, 'murder': 2066, 'murdering': 2067, 'muscular': 2068, 'mushroom': 2069, 'music': 2070, 'must': 2071, 'mustard': 2072, 'mustard-mine': 2073, 'muttered': 2074, 'muttering': 2075, 'my': 2076, 'myself': 2077, 'name': 2078, 'names': 2079, 'narrow': 2080, 'nasty': 2081, 'natural': 2082, 'natured': 2083, 'near': 2084, 'nearer': 2085, 'nearly': 2086, 'neat': 2087, 'neatly': 2088, 'neck': 2089, 'needn': 2090, 'needs': 2091, 'neighbour': 2092, 'neighbouring': 2093, 'neither': 2094, 'nervous': 2095, 'nest': 2096, 'never': 2097, 'never-ending': 2098, 'nevertheless': 2099, 'new': 2100, 'newspapers': 2101, 'next': 2102, 'nibbled': 2103, 'nibbling': 2104, 'nice': 2105, 'nicely': 2106, 'night': 2107, 'night-air': 2108, 'nine': 2109, 'no': 2110, 'nobody': 2111, 'nodded': 2112, 'noise': 2113, 'noises': 2114, 'none': 2115, 'nonsense': 2116, 'nor': 2117, 'nose': 2118, 'not': 2119, 'note-book': 2120, 'nothing': 2121, 'notice': 2122, 'noticed': 2123, 'noticing': 2124, 'notion': 2125, 'now': 2126, 'nowhere': 2127, 'number': 2128, 'nurse': 2129, 'nursing': 2130, 'o': 2131, 'obliged': 2132, 'oblong': 2133, 'obstacle': 2134, 'occasional': 2135, 'occasionally': 2136, 'occurred': 2137, 'odd': 2138, 'of': 2139, 'off': 2140, 'offend': 2141, 'offended': 2142, 'offer': 2143, 'officer': 2144, 'officers': 2145, 'often': 2146, 'oh': 2147, 'ointment': 2148, 'old': 2149, 'older': 2150, 'oldest': 2151, 'on': 2152, 'once': 2153, 'one': 2154, 'ones': 2155, 'oneself': 2156, 'onions': 2157, 'only': 2158, 'oop': 2159, 'ootiful': 2160, 'open': 2161, 'opened': 2162, 'opening': 2163, 'opinion': 2164, 'opportunity': 2165, 'opposite': 2166, 'or': 2167, 'order': 2168, 'ordered': 2169, 'ordering': 2170, 'ornamented': 2171, 'other': 2172, 'others': 2173, 'otherwise': 2174, 'ought': 2175, 'our': 2176, 'ourselves': 2177, 'out': 2178, 'out-of-the-way': 2179, 'outside': 2180, 'over': 2181, 'overcome': 2182, 'overhead': 2183, 'own': 2184, 'oyster': 2185, 'pace': 2186, 'pack': 2187, 'paint': 2188, 'painting': 2189, 'pair': 2190, 'pairs': 2191, 'pale': 2192, 'panted': 2193, 'panting': 2194, 'paper': 2195, 'parchment': 2196, 'pardon': 2197, 'pardoned': 2198, 'part': 2199, 'particular': 2200, 'partner': 2201, 'partners': 2202, 'parts': 2203, 'party': 2204, 'pass': 2205, 'passage': 2206, 'passed': 2207, 'passing': 2208, 'passion': 2209, 'passionate': 2210, 'past': 2211, 'patience': 2212, 'patiently': 2213, 'patriotic': 2214, 'patted': 2215, 'pattering': 2216, 'pattern': 2217, 'pause': 2218, 'paused': 2219, 'paw': 2220, 'paws': 2221, 'pebbles': 2222, 'peeped': 2223, 'peeping': 2224, 'peering': 2225, 'pegs': 2226, 'pence': 2227, 'pencil': 2228, 'pencils': 2229, 'people': 2230, 'pepper': 2231, 'pepper-box': 2232, 'perfectly': 2233, 'perhaps': 2234, 'permitted': 2235, 'persisted': 2236, 'person': 2237, 'personal': 2238, 'pet': 2239, 'picked': 2240, 'picking': 2241, 'picture': 2242, 'pictured': 2243, 'pictures': 2244, 'pie': 2245, 'pie-crust': 2246, 'piece': 2247, 'pieces': 2248, 'pig': 2249, 'pig-baby': 2250, 'pigeon': 2251, 'pigs': 2252, 'pinch': 2253, 'pinched': 2254, 'pine-apple': 2255, 'pink': 2256, 'piteous': 2257, 'pitied': 2258, 'pity': 2259, 'place': 2260, 'placed': 2261, 'places': 2262, 'plainly': 2263, 'plan': 2264, 'planning': 2265, 'plate': 2266, 'plates': 2267, 'play': 2268, 'played': 2269, 'players': 2270, 'playing': 2271, 'pleaded': 2272, 'pleasant': 2273, 'pleasanter': 2274, 'please': 2275, 'pleased': 2276, 'pleases': 2277, 'pleasing': 2278, 'pleasure': 2279, 'plenty': 2280, 'pocket': 2281, 'pointed': 2282, 'pointing': 2283, 'poison': 2284, 'poker': 2285, 'poky': 2286, 'politely': 2287, 'pool': 2288, 'poor': 2289, 'pop': 2290, 'pope': 2291, 'porpoise': 2292, 'position': 2293, 'positively': 2294, 'possible': 2295, 'possibly': 2296, 'pounds': 2297, 'pour': 2298, 'poured': 2299, 'powdered': 2300, 'practice': 2301, 'present': 2302, 'presented': 2303, 'presents': 2304, 'pressed': 2305, 'pressing': 2306, 'pretend': 2307, 'pretending': 2308, 'pretexts': 2309, 'prettier': 2310, 'pretty': 2311, 'prevent': 2312, 'printed': 2313, 'prison': 2314, 'prisoner': 2315, 'prize': 2316, 'prizes': 2317, 'proceed': 2318, 'procession': 2319, 'processions': 2320, 'produced': 2321, 'producing': 2322, 'promise': 2323, 'promised': 2324, 'promising': 2325, 'pronounced': 2326, 'proper': 2327, 'proposal': 2328, 'prosecute': 2329, 'protection': 2330, 'proud': 2331, 'prove': 2332, 'proved': 2333, 'proves': 2334, 'provoking': 2335, 'puffed': 2336, 'pulled': 2337, 'pulling': 2338, 'pun': 2339, 'punching': 2340, 'punished': 2341, 'puppy': 2342, 'purple': 2343, 'purpose': 2344, 'purring': 2345, 'push': 2346, 'put': 2347, 'putting': 2348, 'puzzle': 2349, 'puzzled': 2350, 'puzzling': 2351, 'quarrel': 2352, 'quarrelled': 2353, 'quarrelling': 2354, 'queer': 2355, 'queer-': 2356, 'queer-looking': 2357, 'queerest': 2358, 'question': 2359, 'questions': 2360, 'quick': 2361, 'quicker': 2362, 'quickly': 2363, 'quiet': 2364, 'quietly': 2365, 'quite': 2366, 'quiver': 2367, 'rabbit': 2368, 'rabbit-hole': 2369, 'rabbits': 2370, 'race': 2371, 'race-course': 2372, 'railway': 2373, 'raised': 2374, 'raising': 2375, 'ran': 2376, 'rapidly': 2377, 'rapped': 2378, 'rat-hole': 2379, 'rate': 2380, 'rather': 2381, 'rats': 2382, 'rattle': 2383, 'rattling': 2384, 'raven': 2385, 'ravens': 2386, 'raving': 2387, 'raw': 2388, 're': 2389, 'reach': 2390, 'reaching': 2391, 'read': 2392, 'readily': 2393, 'reading': 2394, 'ready': 2395, 'real': 2396, 'reality': 2397, 'really': 2398, 'rearing': 2399, 'reason': 2400, 'reasonable': 2401, 'reasons': 2402, 'received': 2403, 'recognised': 2404, 'recovered': 2405, 'red': 2406, 'red-hot': 2407, 'reduced': 2408, 'reeds': 2409, 'refreshments': 2410, 'refused': 2411, 'regular': 2412, 'relief': 2413, 'relieved': 2414, 'remain': 2415, 'remained': 2416, 'remaining': 2417, 'remark': 2418, 'remarkable': 2419, 'remarked': 2420, 'remarking': 2421, 'remarks': 2422, 'remedies': 2423, 'remember': 2424, 'remembered': 2425, 'remembering': 2426, 'reminding': 2427, 'removed': 2428, 'repeat': 2429, 'repeated': 2430, 'repeating': 2431, 'replied': 2432, 'reply': 2433, 'resource': 2434, 'respect': 2435, 'respectable': 2436, 'respectful': 2437, 'rest': 2438, 'resting': 2439, 'result': 2440, 'retire': 2441, 'returned': 2442, 'returning': 2443, 'rich': 2444, 'riddle': 2445, 'riddles': 2446, 'ridge': 2447, 'ridges': 2448, 'ridiculous': 2449, 'right': 2450, 'right-hand': 2451, 'righthand': 2452, 'rightly': 2453, 'ring': 2454, 'ringlets': 2455, 'riper': 2456, 'rippling': 2457, 'rise': 2458, 'rises': 2459, 'rising': 2460, 'roared': 2461, 'roast': 2462, 'rock': 2463, 'roof': 2464, 'room': 2465, 'roots': 2466, 'rope': 2467, 'rose': 2468, 'rose-tree': 2469, 'roses': 2470, 'rosetree': 2471, 'roughly': 2472, 'round': 2473, 'row': 2474, 'royal': 2475, 'rubbed': 2476, 'rubbing': 2477, 'rude': 2478, 'rudeness': 2479, 'rule': 2480, 'rules': 2481, 'rumbling': 2482, 'run': 2483, 'running': 2484, 'rush': 2485, 'rushed': 2486, 'rustled': 2487, 'rustling': 2488, 's': 2489, 'sad': 2490, 'sadly': 2491, 'safe': 2492, 'sage': 2493, 'said': 2494, 'salmon': 2495, 'salt': 2496, 'same': 2497, 'sand': 2498, 'sands': 2499, 'sang': 2500, 'sat': 2501, 'saucepan': 2502, 'saucepans': 2503, 'saucer': 2504, 'savage': 2505, 'save': 2506, 'saves': 2507, 'saw': 2508, 'say': 2509, 'saying': 2510, 'says': 2511, 'scale': 2512, 'scaly': 2513, 'school': 2514, 'schoolroom': 2515, 'scolded': 2516, 'scrambling': 2517, 'scratching': 2518, 'scream': 2519, 'screamed': 2520, 'screaming': 2521, 'scroll': 2522, 'sea': 2523, 'sea-shore': 2524, 'search': 2525, 'seaside': 2526, 'seated': 2527, 'second': 2528, 'secondly': 2529, 'secret': 2530, 'see': 2531, 'seeing': 2532, 'seem': 2533, 'seemed': 2534, 'seems': 2535, 'seen': 2536, 'seldom': 2537, 'sell': 2538, 'send': 2539, 'sending': 2540, 'sensation': 2541, 'sense': 2542, 'sent': 2543, 'sentence': 2544, 'sentenced': 2545, 'series': 2546, 'seriously': 2547, 'serpent': 2548, 'serpents': 2549, 'set': 2550, 'setting': 2551, 'settle': 2552, 'settled': 2553, 'settling': 2554, 'seven': 2555, 'several': 2556, 'severely': 2557, 'severity': 2558, 'sh': 2559, 'shade': 2560, 'shake': 2561, 'shaking': 2562, 'shall': 2563, 'shan': 2564, 'shape': 2565, 'shaped': 2566, 'share': 2567, 'shared': 2568, 'sharing': 2569, 'sharks': 2570, 'sharp': 2571, 'sharply': 2572, 'she': 2573, 'shedding': 2574, 'sheep-': 2575, 'shelves': 2576, 'shepherd': 2577, 'shifting': 2578, 'shilling': 2579, 'shillings': 2580, 'shingle': 2581, 'shining': 2582, 'shiny': 2583, 'shiver': 2584, 'shock': 2585, 'shoes': 2586, 'shook': 2587, 'shore': 2588, 'short': 2589, 'shorter': 2590, 'should': 2591, 'shoulder': 2592, 'shoulders': 2593, 'shouldn': 2594, 'shouted': 2595, 'shouting': 2596, 'show': 2597, 'shower': 2598, 'showing': 2599, 'shriek': 2600, 'shrieked': 2601, 'shrieks': 2602, 'shrill': 2603, 'shrimp': 2604, 'shrink': 2605, 'shrinking': 2606, 'shut': 2607, 'shutting': 2608, 'shyly': 2609, 'side': 2610, 'sides': 2611, 'sigh': 2612, 'sighed': 2613, 'sighing': 2614, 'sight': 2615, 'sign': 2616, 'signed': 2617, 'signifies': 2618, 'signify': 2619, 'silence': 2620, 'silent': 2621, 'simple': 2622, 'simpleton': 2623, 'simply': 2624, 'since': 2625, 'sing': 2626, 'singers': 2627, 'singing': 2628, 'sink': 2629, 'sir': 2630, 'sister': 2631, 'sisters': 2632, 'sit': 2633, 'sits': 2634, 'sitting': 2635, 'six': 2636, 'sixpence': 2637, 'size': 2638, 'sizes': 2639, 'skimming': 2640, 'skirt': 2641, 'skurried': 2642, 'sky': 2643, 'sky-rocket': 2644, 'slate': 2645, 'slate-pencil': 2646, 'slates': 2647, 'sleep': 2648, 'sleepy': 2649, 'slightest': 2650, 'slipped': 2651, 'slippery': 2652, 'slowly': 2653, 'small': 2654, 'smaller': 2655, 'smallest': 2656, 'smile': 2657, 'smiled': 2658, 'smiling': 2659, 'smoke': 2660, 'smoking': 2661, 'snail': 2662, 'snappishly': 2663, 'snatch': 2664, 'sneeze': 2665, 'sneezed': 2666, 'sneezes': 2667, 'sneezing': 2668, 'snorting': 2669, 'snout': 2670, 'so': 2671, 'sob': 2672, 'sobbed': 2673, 'sobbing': 2674, 'sobs': 2675, 'soft': 2676, 'softly': 2677, 'soldier': 2678, 'soldiers': 2679, 'solemn': 2680, 'solemnly': 2681, 'solid': 2682, 'some': 2683, 'somebody': 2684, 'somehow': 2685, 'someone': 2686, 'somersault': 2687, 'something': 2688, 'sometimes': 2689, 'somewhere': 2690, 'son': 2691, 'song': 2692, 'soon': 2693, 'sooner': 2694, 'soothing': 2695, 'sorrow': 2696, 'sorrowful': 2697, 'sorrows': 2698, 'sorry': 2699, 'sort': 2700, 'sorts': 2701, 'sound': 2702, 'sounded': 2703, 'sounds': 2704, 'soup': 2705, 'sour': 2706, 'spades': 2707, 'speak': 2708, 'speaker': 2709, 'speaking': 2710, 'spectacles': 2711, 'speech': 2712, 'speed': 2713, 'spell': 2714, 'spite': 2715, 'splash': 2716, 'splashed': 2717, 'splashing': 2718, 'splendidly': 2719, 'spoke': 2720, 'spoken': 2721, 'spoon': 2722, 'spot': 2723, 'sprawling': 2724, 'spread': 2725, 'spreading': 2726, 'squeaked': 2727, 'squeaking': 2728, 'squeeze': 2729, 'squeezed': 2730, 'stairs': 2731, 'stalk': 2732, 'stamping': 2733, 'stand': 2734, 'standing': 2735, 'star-fish': 2736, 'staring': 2737, 'started': 2738, 'startled': 2739, 'state': 2740, 'station': 2741, 'stay': 2742, 'stays': 2743, 'steady': 2744, 'steam-engine': 2745, 'sternly': 2746, 'stick': 2747, 'sticks': 2748, 'stiff': 2749, 'still': 2750, 'stingy': 2751, 'stirring': 2752, 'stockings': 2753, 'stole': 2754, 'stood': 2755, 'stool': 2756, 'stoop': 2757, 'stop': 2758, 'stopped': 2759, 'stopping': 2760, 'story': 2761, 'straight': 2762, 'straightened': 2763, 'straightening': 2764, 'strange': 2765, 'strength': 2766, 'stretched': 2767, 'stretching': 2768, 'strings': 2769, 'struck': 2770, 'stuff': 2771, 'stupid': 2772, 'stupidest': 2773, 'stupidly': 2774, 'subdued': 2775, 'subject': 2776, 'subjects': 2777, 'submitted': 2778, 'succeeded': 2779, 'such': 2780, 'sudden': 2781, 'suddenly': 2782, 'suet': 2783, 'sugar': 2784, 'suit': 2785, 'sulkily': 2786, 'sulky': 2787, 'summer': 2788, 'sun': 2789, 'supple': 2790, 'suppose': 2791, 'suppressed': 2792, 'sure': 2793, 'surprise': 2794, 'surprised': 2795, 'swallow': 2796, 'swallowed': 2797, 'swallowing': 2798, 'swam': 2799, 'sweet-tempered': 2800, 'swim': 2801, 'swimming': 2802, 't': 2803, 'table': 2804, 'tail': 2805, 'tails': 2806, 'take': 2807, 'taken': 2808, 'takes': 2809, 'taking': 2810, 'tale': 2811, 'talk': 2812, 'talking': 2813, 'taller': 2814, 'tarts': 2815, 'taste': 2816, 'tasted': 2817, 'tastes': 2818, 'taught': 2819, 'tea': 2820, 'tea-party': 2821, 'tea-things': 2822, 'tea-time': 2823, 'tea-tray': 2824, 'teaching': 2825, 'teacup': 2826, 'teacups': 2827, 'teapot': 2828, 'tears': 2829, 'teases': 2830, 'teeth': 2831, 'telescope': 2832, 'telescopes': 2833, 'tell': 2834, 'telling': 2835, 'tells': 2836, 'temper': 2837, 'ten': 2838, 'terms': 2839, 'terribly': 2840, 'terrier': 2841, 'terror': 2842, 'than': 2843, 'thank': 2844, 'thanked': 2845, 'that': 2846, 'thatched': 2847, 'the': 2848, 'their': 2849, 'theirs': 2850, 'them': 2851, 'themselves': 2852, 'then': 2853, 'there': 2854, 'these': 2855, 'they': 2856, 'thick': 2857, 'thimble': 2858, 'thin': 2859, 'thing': 2860, 'things': 2861, 'think': 2862, 'thinking': 2863, 'thirteen': 2864, 'this': 2865, 'thistle': 2866, 'thoroughly': 2867, 'those': 2868, 'though': 2869, 'thought': 2870, 'thoughtfully': 2871, 'thoughts': 2872, 'thousand': 2873, 'three': 2874, 'three-legged': 2875, 'threw': 2876, 'throat': 2877, 'throne': 2878, 'through': 2879, 'throw': 2880, 'throwing': 2881, 'thrown': 2882, 'thump': 2883, 'thunder': 2884, 'thunderstorm': 2885, 'tide': 2886, 'tidy': 2887, 'tie': 2888, 'tied': 2889, 'tight': 2890, 'till': 2891, 'time': 2892, 'times': 2893, 'timid': 2894, 'timidly': 2895, 'timo': 2896, 'tinkling': 2897, 'tiny': 2898, 'tipped': 2899, 'tiptoe': 2900, 'tired': 2901, 'tis': 2902, 'tittered': 2903, 'to': 2904, 'to-day': 2905, 'to-night': 2906, 'toast': 2907, 'today': 2908, 'toes': 2909, 'toffee': 2910, 'together': 2911, 'told': 2912, 'tomorrow': 2913, 'tone': 2914, 'tones': 2915, 'tongue': 2916, 'too': 2917, 'took': 2918, 'top': 2919, 'tops': 2920, 'toss': 2921, 'tossing': 2922, 'touch': 2923, 'tougher': 2924, 'towards': 2925, 'toys': 2926, 'trampled': 2927, 'treacle': 2928, 'treacle-well': 2929, 'treading': 2930, 'treat': 2931, 'treated': 2932, 'tree': 2933, 'trees': 2934, 'tremble': 2935, 'trembled': 2936, 'trembling': 2937, 'tremulous': 2938, 'trial': 2939, 'trials': 2940, 'trickling': 2941, 'tricks': 2942, 'tried': 2943, 'triumphantly': 2944, 'trot': 2945, 'trotting': 2946, 'trouble': 2947, 'true': 2948, 'trumpet': 2949, 'trusts': 2950, 'truth': 2951, 'truthful': 2952, 'try': 2953, 'trying': 2954, 'tucked': 2955, 'tulip-roots': 2956, 'tumbled': 2957, 'tumbling': 2958, 'tunnel': 2959, 'tureen': 2960, 'turkey': 2961, 'turn': 2962, 'turn-up': 2963, 'turned': 2964, 'turning': 2965, 'turns': 2966, 'turtles': 2967, 'tut': 2968, 'twelfth': 2969, 'twelve': 2970, 'twentieth': 2971, 'twenty': 2972, 'twenty-four': 2973, 'twice': 2974, 'twinkle': 2975, 'twinkled': 2976, 'twinkling': 2977, 'twist': 2978, 'two': 2979, 'uglify': 2980, 'uglifying': 2981, 'ugly': 2982, 'unable': 2983, 'uncivil': 2984, 'uncomfortable': 2985, 'uncomfortably': 2986, 'uncommon': 2987, 'uncommonly': 2988, 'uncorked': 2989, 'under': 2990, 'underneath': 2991, 'understand': 2992, 'understood': 2993, 'undertone': 2994, 'undo': 2995, 'undoing': 2996, 'uneasily': 2997, 'uneasy': 2998, 'unfolded': 2999, 'unfortunate': 3000, 'unhappy': 3001, 'unimportant': 3002, 'unjust': 3003, 'unless': 3004, 'unlocking': 3005, 'unpleasant': 3006, 'unrolled': 3007, 'until': 3008, 'untwist': 3009, 'unusually': 3010, 'unwillingly': 3011, 'up': 3012, 'upon': 3013, 'upright': 3014, 'upset': 3015, 'upsetting': 3016, 'upstairs': 3017, 'us': 3018, 'use': 3019, 'used': 3020, 'useful': 3021, 'using': 3022, 'usual': 3023, 'usually': 3024, 'usurpation': 3025, 'vague': 3026, 'vanished': 3027, 'vanishing': 3028, 'variations': 3029, 'various': 3030, 've': 3031, 'vegetable': 3032, 'velvet': 3033, 'venture': 3034, 'ventured': 3035, 'verdict': 3036, 'verse': 3037, 'verses': 3038, 'very': 3039, 'vinegar': 3040, 'violence': 3041, 'violent': 3042, 'violently': 3043, 'voice': 3044, 'voices': 3045, 'vote': 3046, 'vulgar': 3047, 'wag': 3048, 'wags': 3049, 'waist': 3050, 'waistcoat-pocket': 3051, 'wait': 3052, 'waited': 3053, 'waiting': 3054, 'walk': 3055, 'walked': 3056, 'walking': 3057, 'walrus': 3058, 'wander': 3059, 'wandered': 3060, 'wandering': 3061, 'want': 3062, 'wanted': 3063, 'wants': 3064, 'warning': 3065, 'was': 3066, 'wash': 3067, 'washing': 3068, 'wasn': 3069, 'waste': 3070, 'wasting': 3071, 'watch': 3072, 'watched': 3073, 'watching': 3074, 'water': 3075, 'water-well': 3076, 'waters': 3077, 'waving': 3078, 'way': 3079, 'ways': 3080, 'we': 3081, 'weak': 3082, 'wearily': 3083, 'week': 3084, 'weeks': 3085, 'welcome': 3086, 'well': 3087, 'went': 3088, 'wept': 3089, 'were': 3090, 'weren': 3091, 'wet': 3092, 'what': 3093, 'whatever': 3094, 'when': 3095, 'where': 3096, 'whereupon': 3097, 'wherever': 3098, 'whether': 3099, 'which': 3100, 'while': 3101, 'whiles': 3102, 'whiskers': 3103, 'whisper': 3104, 'whispered': 3105, 'whispers': 3106, 'whistle': 3107, 'whistling': 3108, 'white': 3109, 'whiting': 3110, 'who': 3111, 'whole': 3112, 'whom': 3113, 'whose': 3114, 'why': 3115, 'wide': 3116, 'wider': 3117, 'wife': 3118, 'wig': 3119, 'wild': 3120, 'wildly': 3121, 'will': 3122, 'win': 3123, 'wind': 3124, 'window': 3125, 'wine': 3126, 'wings': 3127, 'wink': 3128, 'winter': 3129, 'wise': 3130, 'wish': 3131, 'with': 3132, 'within': 3133, 'without': 3134, 'witness': 3135, 'wits': 3136, 'woke': 3137, 'woman': 3138, 'won': 3139, 'wonder': 3140, 'wondered': 3141, 'wonderful': 3142, 'wondering': 3143, 'wood': 3144, 'wooden': 3145, 'word': 3146, 'words': 3147, 'wore': 3148, 'work': 3149, 'works': 3150, 'world': 3151, 'worm': 3152, 'worried': 3153, 'worry': 3154, 'worse': 3155, 'worth': 3156, 'would': 3157, 'wouldn': 3158, 'wow': 3159, 'wrapping': 3160, 'wretched': 3161, 'wriggling': 3162, 'write': 3163, 'writing': 3164, 'writing-desk': 3165, 'writing-desks': 3166, 'written': 3167, 'wrong': 3168, 'wrote': 3169, 'yards': 3170, 'yawned': 3171, 'yawning': 3172, 'ye': 3173, 'year': 3174, 'years': 3175, 'yelled': 3176, 'yelp': 3177, 'yer': 3178, 'yes': 3179, 'yesterday': 3180, 'yet': 3181, 'you': 3182, 'young': 3183, 'your': 3184, 'yours': 3185, 'yourself': 3186, 'youth': 3187, 'zigzag': 3188}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Toeknizer Class\n"
      ],
      "metadata": {
        "id": "0hRxICBSexyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1():\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {index: word for word, index in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    # Replace spaces before the specified punctuations\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "O3wF-08Xe42B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vcab)\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(f\"Token IDs: {ids}\")\n",
        "decodes = tokenizer.decode(ids)\n",
        "print(f\"Word : {decodes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "o_CE4mzYg39b",
        "outputId": "bb151379-86a1-43ec-cc2c-d25614a8f401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'painted'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1620389208.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m text = \"\"\"\"It's the last he painted, you know,\"\n\u001b[1;32m      3\u001b[0m            Mrs. Gisburn said with pardonable pride.\"\"\"\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Token IDs: {ids}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdecodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-766664413.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([,.:;?_!\"()\\']|--|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'painted'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IF sentance is not poresent in vacob\n",
        "# text = \"Hello, do you like tea?\"\n",
        "# print(tokenizer.encode(text))\n",
        "# SInce Hello is not present, it will throw an error"
      ],
      "metadata": {
        "id": "QYnmblFPiWls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Special Context Token For Missign Word/Token"
      ],
      "metadata": {
        "id": "W6WbcGo1jHXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To better handle this add two tokens, endoftext and unk\n",
        "all_token = sorted(list(set(preprocessed)))\n",
        "all_token.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "vcab_size = len(all_token)\n",
        "print(f\"Vocab size: {vcab_size}\")\n",
        "vocab = {token: integer for integer, token in enumerate(all_token)}\n",
        "print(len(vocab.items()))\n"
      ],
      "metadata": {
        "id": "uKVHEOjIi-bH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36d656d3-7711-41b3-e7fc-8eeb31635bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 3191\n",
            "3191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osA2nTm-42x3",
        "outputId": "83264441-3207-4946-f898-a3311eaffe20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('yourself', 3186)\n",
            "('youth', 3187)\n",
            "('zigzag', 3188)\n",
            "('<|endoftext|>', 3189)\n",
            "('<|unk|>', 3190)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i: s for s, i in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    preprocessed = [\n",
        "        item if item in self.str_to_int\n",
        "        else \"<|unk|>\" for item in preprocessed\n",
        "    ]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    # Replace spaces before the specified punctuations\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "J1jgl22t5KGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BxhnYjM586o",
        "outputId": "4da0a936-a3a5-4afa-e9bc-e4fc577c4d7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYjSu_Wn6cuE",
        "outputId": "dad67f2d-2a24-4312-e61a-7975d4883a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3190,\n",
              " 6,\n",
              " 1289,\n",
              " 3182,\n",
              " 1920,\n",
              " 2820,\n",
              " 25,\n",
              " 3189,\n",
              " 198,\n",
              " 2848,\n",
              " 3190,\n",
              " 3190,\n",
              " 2139,\n",
              " 2848,\n",
              " 3190,\n",
              " 8]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Byte Pair Encoding\n",
        " Not eeach word is a token, it can be broken down inot multiple subword/token\n",
        "\n",
        "\n",
        "* Word based tokenizer\n",
        "* Subword based tokenizer\n",
        "* Character based tokenizer\n",
        "\n",
        "Byte Pair is subword tokenization\n",
        "\n",
        "**tiktoken pythom library will be used**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EeNdj_BS_wxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install tiktoken -q"
      ],
      "metadata": {
        "id": "-ldoHyqUz7nU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "print(f\"tiktoken version: {tiktoken.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FnO_ds70A-D",
        "outputId": "5dc4ee5b-fb26-484d-ce3f-2530dd1a5af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Byte pair tokenizer\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "zwkAbdCd0Opx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## BPT encoding\n",
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "     \"of someunknownPlace.\"\n",
        ")\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1zf9_aI0nFV",
        "outputId": "de627baf-4584-4e0b-9b0f-ecd32f6453e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## BPT decoding\n",
        "words = tokenizer.decode(integers)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGTpDMOu1qWU",
        "outputId": "becc9fa6-9895-45f7-ddab-2e49aa253676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Another experiment for tokenizaton with article text\n",
        "articleTokenIds = tokenizer.encode(text_data)\n",
        "print(f\"articleTokenIds len : {len(articleTokenIds)}\")\n",
        "aticleString = tokenizer.decode(articleTokenIds)\n",
        "print(f\"aticleString len : {len(aticleString)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr6AiBMx3q2h",
        "outputId": "8bdb2708-27f5-4b12-aed9-57e7c55213bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "articleTokenIds len : 42098\n",
            "aticleString len : 148208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INPUT-TARGET PAIRS\n",
        "\n",
        "* implement data loader that fetches the input-target pairs using a sliding window approach"
      ],
      "metadata": {
        "id": "VoFTvR1oKW3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the first 200 token from data set to do experiment\n",
        "articleTokenIds = articleTokenIds[200:]"
      ],
      "metadata": {
        "id": "r8BrGyd64FqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4 # length of the input, model is trained to look at a sequence of 4 words\n",
        "x = articleTokenIds[:context_size]\n",
        "y = articleTokenIds[1:context_size+1]\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y: {y}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWmruCrGMhzf",
        "outputId": "2c7e1d00-3761-496b-f7ef-2748a71f704b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [503, 286, 262, 835]\n",
            "y: [286, 262, 835, 284]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "  context = articleTokenIds[:i]\n",
        "  desired = articleTokenIds[i]\n",
        "  print(f\"context: {context}, desired: {desired}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgaGPokoNNQi",
        "outputId": "4440631e-7ff7-4d9f-8159-b10679322de9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context: [503], desired: 286\n",
            "context: [503, 286], desired: 262\n",
            "context: [503, 286, 262], desired: 835\n",
            "context: [503, 286, 262, 835], desired: 284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "  context = articleTokenIds[:i]\n",
        "  desired = articleTokenIds[i]\n",
        "  print(f\"{tokenizer.decode(context)} ----> {tokenizer.decode([desired])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8iV-v-6NWci",
        "outputId": "af8a29c4-3767-4b70-e5f3-d001c93c85c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " out ---->  of\n",
            " out of ---->  the\n",
            " out of the ---->  way\n",
            " out of the way ---->  to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implement dataloader using PyTorch to convert it into tensor"
      ],
      "metadata": {
        "id": "6DyOdKTMQqsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DataSet\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class GPTDatasetV1(Dataset):\n",
        "  def __init__(self, txt, tokenizer, max_length, stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "    ## Tokenize the etire text\n",
        "    token_id = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "    for i in range(0, len(token_id)-max_length, stride):\n",
        "      input_chunk = token_id[i:i+max_length]\n",
        "      target_chunk = token_id[i+1:i+max_length+1]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.input_ids[index], self.target_ids[index]\n"
      ],
      "metadata": {
        "id": "2nlw6RLnQyDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                      stride=128, shuffle=True, drop_last=True,\n",
        "                      num_workers=0):\n",
        "  # Initialize the tokenizer\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "  # Create the dataset\n",
        "  dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "  dataloader = DataLoader(dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,\n",
        "                          drop_last=drop_last,\n",
        "                          num_workers=num_workers)\n",
        "  return dataloader\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4KMI9h_3dhdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloder = create_dataloader_v1(text_data, batch_size=8,\n",
        "                                 max_length=4,\n",
        "                                 stride=4,\n",
        "                                 shuffle=False,\n",
        "                                 num_workers=2)\n",
        "\n",
        "dataloder_itr = iter(dataloder)\n",
        "first_batch = next(dataloder_itr)\n",
        "print(f\"first_batch:\\n {first_batch}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH3-wwcmeq-u",
        "outputId": "221f1525-de4e-4416-b8be-57c2d5ab5b24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first_batch:\n",
            " [tensor([[  345, 10783,   284,  1560],\n",
            "        [  588,   262,   804,   286],\n",
            "        [  198,    28,  5870, 29485],\n",
            "        [  757, 13679,   628,   220],\n",
            "        [  673,   198, 18108,   587],\n",
            "        [  318,   340,  8348,   628],\n",
            "        [  843,   262, 33958,   746],\n",
            "        [  628,   220,  4600,  8241]]), tensor([[10783,   284,  1560,   502],\n",
            "        [  262,   804,   286,   262],\n",
            "        [   28,  5870, 29485,  6711],\n",
            "        [13679,   628,   220,  4600],\n",
            "        [  198, 18108,   587, 24504],\n",
            "        [  340,  8348,   628,   220],\n",
            "        [  262, 33958,   746,   261],\n",
            "        [  220,  4600,  8241, 15986]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_batch = next(dataloder_itr)\n",
        "print(f\"second_batch:\\n {second_batch}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOpyQlYehtDe",
        "outputId": "1c931153-7bc8-4c79-a1cc-8fa2d9831f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "second_batch:\n",
            " [tensor([[  220,   220,   220,   220],\n",
            "        [  220,   314, 16498,   910],\n",
            "        [   25,   198,    63,   270],\n",
            "        [   13,   220,  4600,    40],\n",
            "        [  220, 24430,   198,   220],\n",
            "        [ 2540, 10868,   606,   510],\n",
            "        [   40,   466,  4601,   314],\n",
            "        [15986,   345,  3375,   284]]), tensor([[  220,   220,   220,   220],\n",
            "        [  314, 16498,   910,   612],\n",
            "        [  198,    63,   270,  6140],\n",
            "        [  220,  4600,    40,  2492],\n",
            "        [24430,   198,   220,   220],\n",
            "        [10868,   606,   510,   757],\n",
            "        [  466,  4601,   314,  8020],\n",
            "        [  345,  3375,   284,  8348]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Token Embeddings\n",
        "\n",
        "* It convert words/token with their associated verctor\n",
        "* Sementic Meanings preserved"
      ],
      "metadata": {
        "id": "jSXTsYO6jmFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example with small embedding\n",
        "example_input_ids = torch.tensor([[2, 3, 5, 1]])\n",
        "example_vocab_size = 6\n",
        "example_dim = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "example_embedding_layer = torch.nn.Embedding(example_vocab_size, example_dim)\n",
        "# example_output = example_embedding_layer(example_input_ids)\n",
        "# print(example_output)\n",
        "print(example_embedding_layer.weight)\n"
      ],
      "metadata": {
        "id": "8_GL8OcQlgAd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1f517b0-2c13-486b-a927-c28dfe4ac890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lookup in embedding matrix\n",
        "print(example_embedding_layer(torch.tensor([3])))\n",
        "print(example_embedding_layer.weight[3])"
      ],
      "metadata": {
        "id": "6oUzujRwlf9v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1babc5b8-6620-4ec1-de0a-1e6af9058d16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n",
            "tensor([-0.4015,  0.9666, -1.1481], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Encoding\n",
        "\n",
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Previously, we focused on very small embedding sizes in this chapter for illustration\n",
        "purposes.\n",
        "\n",
        "We now consider more realistic and useful embedding sizes and encode the input\n",
        "tokens into a 256-dimensional vector representation.\n",
        "\n",
        "This is smaller than what the original\n",
        "GPT-3 model used (in GPT-3, the embedding size is 12,288 dimensions) but still reasonable\n",
        "for experimentation.\n",
        "\n",
        "Furthermore, we assume that the token IDs were created by the BPE\n",
        "tokenizer that we implemented earlier, which has a vocabulary size of 50,257:\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "y0qsg3Rl-iKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "l4AkhBUvlf4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    text_data, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ],
      "metadata": {
        "id": "Q7FJDlwblfxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7gSCLVIsu5L",
        "outputId": "3b14fa47-c23f-471d-9493-37c2f0e01e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[ 1560,   502,  4032,   531],\n",
            "        [  198, 47436,   284,   262],\n",
            "        [  198,    63,   403, 18049],\n",
            "        [  464, 40289,     0,   220],\n",
            "        [  287,   257,  1877,    11],\n",
            "        [  475,    11,   706,  4964],\n",
            "        [13679,   290,   262,  2677],\n",
            "        [ 7898,   262,  4973,   379]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CoEmbed these token ids inot 256 dimentional vectors - 8x4x256\n",
        "token_embeding = token_embedding_layer(inputs)\n",
        "print(token_embeding.shape)\n",
        "# tokeb_embeding[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPemn7SEtY-q",
        "outputId": "5cb89ac3-f5ed-4e76-f318-9fa94aa57daa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### create another embedding layer for positional encoding\n",
        "It would be of size context_length(4)x256"
      ],
      "metadata": {
        "id": "leZ0I-ymuNK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n"
      ],
      "metadata": {
        "id": "YO717PfkuQbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embedding = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embedding.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdO-06wmuzzA",
        "outputId": "04ba29f8-7d6d-4cff-b817-5cee4b9f1d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now add both toekn embeding and pos embeding to get final embeddings\n",
        "input_embeddings = token_embeding + pos_embedding\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeWjuPqgwgcq",
        "outputId": "7e21f976-fae1-4558-dffd-21673268c632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    }
  ]
}