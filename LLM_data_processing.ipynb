{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fresh start for data preparation\n",
        "1. Tokenization\n",
        "2. Toekn Embedding\n",
        "3. Positional Embedding\n",
        "4. Input Embedding (toekn+positional)\n"
      ],
      "metadata": {
        "id": "71EnYx0y_RW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Tokenization\n"
      ],
      "metadata": {
        "id": "N9L4NRx3_zBM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Create tokens"
      ],
      "metadata": {
        "id": "BKm6HHjX_7be"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lMVnSYx-81L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the book\n",
        "# file_path = \"the-verdict.txt\"\n",
        "file_path = \"alice_in_wonderland.txt\"\n",
        "# url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
        "url = \"https://raw.githubusercontent.com/kuemit/txt_book/refs/heads/master/examples/alice_in_wonderland.txt\"\n",
        "if not os.path.exists(file_path):\n",
        "  response = requests.get(url, timeout=30)\n",
        "  response.raise_for_status()\n",
        "  text_data = response.text\n",
        "  with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(text_data)\n",
        "else:\n",
        "  with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    text_data = file.read()\n",
        "\n",
        "print(f\"Length of text: {len(text_data)} characters\")\n",
        "print(f\"First 100 characters of text: {text_data[:99]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOcispU8_9lZ",
        "outputId": "99bcd2c6-034c-481b-918f-808fd3daa367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 148208 characters\n",
            "First 100 characters of text: TITLE: Alice's Adventures in Wonderland\n",
            "AUTHOR: Lewis Carroll\n",
            "\n",
            "\n",
            "= CHAPTER I = \n",
            "=( Down the Rabbit-H\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Word based tokenization to start with"
      ],
      "metadata": {
        "id": "w7lvUb2lAK3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spliting the word\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text_data)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])\n",
        "print(f\"Length of preprocessed: {len(preprocessed)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXCVBFjyAIfR",
        "outputId": "934d7e26-b5a9-4561-de38-e0723d083778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['TITLE', ':', 'Alice', \"'\", 's', 'Adventures', 'in', 'Wonderland', 'AUTHOR', ':', 'Lewis', 'Carroll', '=', 'CHAPTER', 'I', '=', '=', '(', 'Down', 'the', 'Rabbit-Hole', ')', '=', 'Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired']\n",
            "Length of preprocessed: 34158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Crating token ids"
      ],
      "metadata": {
        "id": "ccK3cIGRDHPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "print(f\"Vocab size: {vocab_size}\")\n",
        "vocab = {word: idx for idx, word in enumerate(all_words)}\n",
        "id_to_word = {idx: word for idx, word in enumerate(all_words)}\n",
        "print(f\" word_to_id['Alice']: {vocab['Alice']}\")\n",
        "print(f\" id_to_word[1]: {id_to_word[40]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yptdjr77DKdc",
        "outputId": "1f995737-20ef-4a61-8c18-2907934c9eed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 3189\n",
            " word_to_id['Alice']: 40\n",
            " id_to_word[1]: Alice\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'([,.:;?_!\"()\\']|--|\\s)', r'\\1', text)\n",
        "    return text\n",
        "\n"
      ],
      "metadata": {
        "id": "7KJJ0zA8KRDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "ids = tokenizer.encode(text_data)\n",
        "print(ids[:100])\n",
        "# print(tokenizer.decode(ids[:100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IMD4s0FLOp1",
        "outputId": "114783ef-bcb2-441e-8fa7-3f1e4452fa55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[377, 22, 40, 2, 2489, 35, 1772, 447, 33, 22, 222, 83, 24, 76, 184, 24, 24, 3, 110, 2848, 321, 4, 24, 40, 3066, 913, 2904, 1589, 3039, 2901, 2139, 2635, 1011, 1702, 2631, 2152, 2848, 879, 6, 805, 2139, 1680, 2121, 2904, 1289, 22, 2153, 2167, 2974, 2573, 1649, 2223, 1795, 2848, 954, 1702, 2631, 3066, 2394, 6, 1005, 1806, 1649, 2110, 2244, 2167, 1149, 1772, 1806, 6, 663, 3093, 1804, 2848, 3019, 2139, 736, 954, 6, 2, 2870, 40, 734, 2244, 2167, 1148, 25, 2, 349, 2573, 3066, 1139, 1772, 1702, 2184, 2029, 3, 846, 3087, 846]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(ids[:50]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5E-trKxoLdFg",
        "outputId": "21dca98a-fb85-4563-a8db-0d98daedf1de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TITLE : Alice ' s Adventures in Wonderland AUTHOR : Lewis Carroll = CHAPTER I = = ( Down the Rabbit-Hole ) = Alice was beginning to get very tired of sitting by her sister on the bank , and of having nothing to do : once or twice she\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Add special tokenizer for unknown text handling"
      ],
      "metadata": {
        "id": "hd_OycljL0tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
        "len(vocab.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VV72nRmMTcq",
        "outputId": "601f614f-6ddf-4078-e24e-005975f83cc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3191"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDG0KGa-MXXj",
        "outputId": "c60f3724-220a-4bac-ff83-1f4909a322c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('yourself', 3186)\n",
            "('youth', 3187)\n",
            "('zigzag', 3188)\n",
            "('<|endoftext|>', 3189)\n",
            "('<|unk|>', 3190)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    preprocessed = [\n",
        "        item if item in self.str_to_int\n",
        "        else \"<|unk|>\" for item in preprocessed\n",
        "    ]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'([,.:;?_!\"()\\']|--|\\s)', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "ee5G3U2bMuQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "ids = tokenizer.encode(text_data)\n",
        "print(ids[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcu3fEfINgwr",
        "outputId": "eaadbd8e-c011-4898-e457-b1004b323ce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[377, 22, 40, 2, 2489, 35, 1772, 447, 33, 22, 222, 83, 24, 76, 184, 24, 24, 3, 110, 2848, 321, 4, 24, 40, 3066, 913, 2904, 1589, 3039, 2901, 2139, 2635, 1011, 1702, 2631, 2152, 2848, 879, 6, 805, 2139, 1680, 2121, 2904, 1289, 22, 2153, 2167, 2974, 2573, 1649, 2223, 1795, 2848, 954, 1702, 2631, 3066, 2394, 6, 1005, 1806, 1649, 2110, 2244, 2167, 1149, 1772, 1806, 6, 663, 3093, 1804, 2848, 3019, 2139, 736, 954, 6, 2, 2870, 40, 734, 2244, 2167, 1148, 25, 2, 349, 2573, 3066, 1139, 1772, 1702, 2184, 2029, 3, 846, 3087, 846]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(ids[:50]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oDHnkOzNmkV",
        "outputId": "5610cc40-90c0-43e6-899f-0d53b6da9fb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TITLE : Alice ' s Adventures in Wonderland AUTHOR : Lewis Carroll = CHAPTER I = = ( Down the Rabbit-Hole ) = Alice was beginning to get very tired of sitting by her sister on the bank , and of having nothing to do : once or twice she\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Byte pair encoding"
      ],
      "metadata": {
        "id": "Vx__Jd8m1fED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install tiktoken -q"
      ],
      "metadata": {
        "id": "MDqwGXyx1hKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "print(f\"tiktoken version: {tiktoken.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fp3rV9lQ1lAC",
        "outputId": "2a429f66-5991-4f04-d38d-1f25b62d22cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Byte pair tokenizer\n",
        "tokenizerBPE = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "-qtrXYSa1m1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "integers = tokenizerBPE.encode(text_data, allowed_special={\"<|endoftext|>\"})\n",
        "print(len(integers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlgMbSyE1qY_",
        "outputId": "c6a888ff-63d6-4379-b213-cd955c7c3b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiple ecodings using tiktoken\n",
        "encodings = {\n",
        "    \"gpt2\": tiktoken.get_encoding(\"gpt2\"),\n",
        "    \"gpt3\": tiktoken.get_encoding(\"p50k_base\"),\n",
        "    \"gpt4\": tiktoken.get_encoding(\"cl100k_base\")\n",
        "}\n",
        "vocab_sizes = {model: encoding.n_vocab for model, encoding in encodings.items()}\n",
        "# print\n",
        "for model, vocab_size in vocab_sizes.items():\n",
        "  print(f\"vocabulary size for {model.upper()}: {vocab_size}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AasmXNKv2ZTn",
        "outputId": "5ea04f60-45c0-403e-e549-aab802717854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary size for GPT2: 50257\n",
            "vocabulary size for GPT3: 50281\n",
            "vocabulary size for GPT4: 100277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Token Embedding"
      ],
      "metadata": {
        "id": "Eu98Qkc63wz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Input-Target pairs"
      ],
      "metadata": {
        "id": "8vIalteZ7N9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_text = tokenizerBPE.encode(text_data, allowed_special={\"<|endoftext|>\"})\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjYR_6u77UtN",
        "outputId": "bd168a5b-7a25-4706-b224-a83d999d3067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4 #length of the input\n",
        "#The context_size of 4 means that the model is trained to look at a sequence of 4 words (or tokens)\n",
        "#to predict the next word in the sequence.\n",
        "#The input x is the first 4 tokens [1, 2, 3, 4], and the target y is the next 4 tokens [2, 3, 4, 5]\n",
        "\n",
        "x = enc_text[:context_size]\n",
        "y = enc_text[1:context_size+1]\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_3wRBdX7ksF",
        "outputId": "44108424-9c52-463e-ab98-e1e7989f34a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [49560, 2538, 25, 14862]\n",
            "y:      [2538, 25, 14862, 338]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_text[:i]\n",
        "    desired = enc_text[i]\n",
        "\n",
        "    print(context, \"---->\", desired)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o93FmWW7xM9",
        "outputId": "ef9528ba-0451-4827-b168-0ea1d8605cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[49560] ----> 2538\n",
            "[49560, 2538] ----> 25\n",
            "[49560, 2538, 25] ----> 14862\n",
            "[49560, 2538, 25, 14862] ----> 338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_text[:i]\n",
        "    desired = enc_text[i]\n",
        "\n",
        "    print(tokenizerBPE.decode(context), \"---->\", tokenizerBPE.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kzt5T9bG76sL",
        "outputId": "0de8aead-b953-4d90-ed3e-866216a758c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TIT ----> LE\n",
            "TITLE ----> :\n",
            "TITLE: ---->  Alice\n",
            "TITLE: Alice ----> 's\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Data Loader"
      ],
      "metadata": {
        "id": "1OftB3OR8BY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class GPTDatasetV1(Dataset):\n",
        "  def __init__(self, txt, tokenizer, max_length, stride):\n",
        "    self.input_id = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    tokend_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "    for i in range(0, len(tokend_ids)-max_length, stride):\n",
        "      input_chunk = tokend_ids[i:i+max_length]\n",
        "      target_chunk = tokend_ids[i+1:i+max_length+1]\n",
        "      self.input_id.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_id)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_id[idx], self.target_ids[idx]\n",
        "\n"
      ],
      "metadata": {
        "id": "3vbFsDi58DRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(text, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "  dataset = GPTDatasetV1(text, tokenizer, max_length, stride)\n",
        "\n",
        "  dataloader = DataLoader(dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=shuffle,\n",
        "                          drop_last=drop_last,\n",
        "                          num_workers=num_workers)\n",
        "  return dataloader\n"
      ],
      "metadata": {
        "id": "O8iyUyZ2-6D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    text_data, batch_size=8, max_length=4, stride=4, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azKX-xDL_il8",
        "outputId": "356e5814-3db9-4598-ad35-b2b7c819e294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cpu\n",
            "Inputs:\n",
            " tensor([[49560,  2538,    25, 14862],\n",
            "        [  338, 15640,   287, 42713],\n",
            "        [  198,    32, 24318,  1581],\n",
            "        [   25, 10174, 21298,   628],\n",
            "        [  198,    28,  5870, 29485],\n",
            "        [  314,   796,   220,   198],\n",
            "        [16193,  5588,   262, 25498],\n",
            "        [   12,    39,  2305,  1267]])\n",
            "\n",
            "Targets:\n",
            " tensor([[ 2538,    25, 14862,   338],\n",
            "        [15640,   287, 42713,   198],\n",
            "        [   32, 24318,  1581,    25],\n",
            "        [10174, 21298,   628,   198],\n",
            "        [   28,  5870, 29485,   314],\n",
            "        [  796,   220,   198, 16193],\n",
            "        [ 5588,   262, 25498,    12],\n",
            "        [   39,  2305,  1267,    28]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "# torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "embedding_layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09qPSeIfDl1V",
        "outputId": "0c2f8b44-8b49-498e-c6f7-c897f2a56646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50257, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = embedding_layer(inputs)\n",
        "token_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlyz-WDPElgd",
        "outputId": "9ef24517-3e63-4270-aac1-ba6ceef5eb17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 4, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Positional Embedding"
      ],
      "metadata": {
        "id": "CMWuUZkaE3pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
      ],
      "metadata": {
        "id": "V2dRlDdSKGvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZuswPPFKTQb",
        "outputId": "2ed6b155-53d4-41b3-e216-909b50bcf70b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Input Embedding"
      ],
      "metadata": {
        "id": "CazmLD-uKZs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embedding + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0lz2XA8KiKi",
        "outputId": "5389d263-e13f-4212-f5ee-442df1336c24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    }
  ]
}